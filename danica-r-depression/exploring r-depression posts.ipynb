{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and loading in /r/depression posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danicatanquilut/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (31,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "posts = pd.read_csv(\"depression_posts_full_corpus_201509.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_texts = posts['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrive given stop word set\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of personal pronouns found in given stop word set\n",
    "personal_pronouns = ['i', 'i\\'d', 'i\\'ll', 'i\\'m', 'i\\'ve',\n",
    "                      'me', 'my','myself']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing personal pronouns from stop word array\n",
    "for word in personal_pronouns:\n",
    "    en_stop.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out nltk's tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197436\n"
     ]
    }
   ],
   "source": [
    "num_posts = len(post_texts)\n",
    "print(num_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens have already been created -- Do not need to re-run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating tweet tokenizer\n",
    "tweet_tokenizer = nltk.tokenize.casual.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create tokens using tweet tokenizer\n",
    "new_tokens = []\n",
    "full_tokens = []\n",
    "for x in np.arange(num_posts):\n",
    "    if type(post_texts[x]) != float:\n",
    "        new_tokens = tweet_tokenizer.tokenize(post_texts[x])\n",
    "        stopped_tokens = [i for i in new_tokens if not i in en_stop]\n",
    "        full_tokens.append(stopped_tokens)\n",
    "    if  ((x + 1) % 2000 == 0) | (x == num_posts - 1):\n",
    "        # write to file\n",
    "        file_obj = open('filename_%s.pkl' % x, 'wb')\n",
    "        pkl.dump(full_tokens, file_obj) \n",
    "        full_tokens = []\n",
    "        print('Wrote to filename_%s.pkl' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Stemmer\n",
    "Stemmed words have already been stored in files - do not need to re-run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_stemmer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed stemmed_1999.pkl\n",
      "Stemmed stemmed_3999.pkl\n",
      "Stemmed stemmed_5999.pkl\n",
      "Stemmed stemmed_7999.pkl\n",
      "Stemmed stemmed_9999.pkl\n",
      "Stemmed stemmed_11999.pkl\n",
      "Stemmed stemmed_13999.pkl\n",
      "Stemmed stemmed_15999.pkl\n",
      "Stemmed stemmed_17999.pkl\n",
      "Stemmed stemmed_19999.pkl\n",
      "Stemmed stemmed_21999.pkl\n",
      "Stemmed stemmed_23999.pkl\n",
      "Stemmed stemmed_25999.pkl\n",
      "Stemmed stemmed_27999.pkl\n",
      "Stemmed stemmed_29999.pkl\n",
      "Stemmed stemmed_31999.pkl\n",
      "Stemmed stemmed_33999.pkl\n",
      "Stemmed stemmed_35999.pkl\n",
      "Stemmed stemmed_37999.pkl\n",
      "Stemmed stemmed_39999.pkl\n",
      "Stemmed stemmed_41999.pkl\n",
      "Stemmed stemmed_43999.pkl\n",
      "Stemmed stemmed_45999.pkl\n",
      "Stemmed stemmed_47999.pkl\n",
      "Stemmed stemmed_49999.pkl\n",
      "Stemmed stemmed_51999.pkl\n",
      "Stemmed stemmed_53999.pkl\n",
      "Stemmed stemmed_55999.pkl\n",
      "Stemmed stemmed_57999.pkl\n",
      "Stemmed stemmed_59999.pkl\n",
      "Stemmed stemmed_61999.pkl\n",
      "Stemmed stemmed_63999.pkl\n",
      "Stemmed stemmed_65999.pkl\n",
      "Stemmed stemmed_67999.pkl\n",
      "Stemmed stemmed_69999.pkl\n",
      "Stemmed stemmed_71999.pkl\n",
      "Stemmed stemmed_73999.pkl\n",
      "Stemmed stemmed_75999.pkl\n",
      "Stemmed stemmed_77999.pkl\n",
      "Stemmed stemmed_79999.pkl\n",
      "Stemmed stemmed_81999.pkl\n",
      "Stemmed stemmed_83999.pkl\n",
      "Stemmed stemmed_85999.pkl\n",
      "Stemmed stemmed_87999.pkl\n",
      "Stemmed stemmed_89999.pkl\n",
      "Stemmed stemmed_91999.pkl\n",
      "Stemmed stemmed_93999.pkl\n",
      "Stemmed stemmed_95999.pkl\n",
      "Stemmed stemmed_97999.pkl\n",
      "Stemmed stemmed_99999.pkl\n",
      "Stemmed stemmed_101999.pkl\n",
      "Stemmed stemmed_103999.pkl\n",
      "Stemmed stemmed_105999.pkl\n",
      "Stemmed stemmed_107999.pkl\n",
      "Stemmed stemmed_109999.pkl\n",
      "Stemmed stemmed_111999.pkl\n",
      "Stemmed stemmed_113999.pkl\n",
      "Stemmed stemmed_115999.pkl\n",
      "Stemmed stemmed_117999.pkl\n",
      "Stemmed stemmed_119999.pkl\n",
      "Stemmed stemmed_121999.pkl\n",
      "Stemmed stemmed_123999.pkl\n",
      "Stemmed stemmed_125999.pkl\n",
      "Stemmed stemmed_127999.pkl\n",
      "Stemmed stemmed_129999.pkl\n",
      "Stemmed stemmed_131999.pkl\n",
      "Stemmed stemmed_133999.pkl\n",
      "Stemmed stemmed_135999.pkl\n",
      "Stemmed stemmed_137999.pkl\n",
      "Stemmed stemmed_139999.pkl\n",
      "Stemmed stemmed_141999.pkl\n",
      "Stemmed stemmed_143999.pkl\n",
      "Stemmed stemmed_145999.pkl\n",
      "Stemmed stemmed_147999.pkl\n",
      "Stemmed stemmed_149999.pkl\n",
      "Stemmed stemmed_151999.pkl\n",
      "Stemmed stemmed_153999.pkl\n",
      "Stemmed stemmed_155999.pkl\n",
      "Stemmed stemmed_157999.pkl\n",
      "Stemmed stemmed_159999.pkl\n",
      "Stemmed stemmed_161999.pkl\n",
      "Stemmed stemmed_163999.pkl\n",
      "Stemmed stemmed_165999.pkl\n",
      "Stemmed stemmed_167999.pkl\n",
      "Stemmed stemmed_169999.pkl\n",
      "Stemmed stemmed_171999.pkl\n",
      "Stemmed stemmed_173999.pkl\n",
      "Stemmed stemmed_175999.pkl\n",
      "Stemmed stemmed_177999.pkl\n",
      "Stemmed stemmed_179999.pkl\n",
      "Stemmed stemmed_181999.pkl\n",
      "Stemmed stemmed_183999.pkl\n",
      "Stemmed stemmed_185999.pkl\n",
      "Stemmed stemmed_187999.pkl\n",
      "Stemmed stemmed_189999.pkl\n",
      "Stemmed stemmed_191999.pkl\n",
      "Stemmed stemmed_193999.pkl\n",
      "Stemmed stemmed_195999.pkl\n",
      "Stemmed stemmed_197435.pkl\n"
     ]
    }
   ],
   "source": [
    "#stemming tokens using wordnet stemmer (lemmatizer?)\n",
    "for x in np.arange(num_posts):\n",
    "    if  ((x + 1) % 2000 == 0) | (x == num_posts - 1):\n",
    "        read_file = pkl.load(open('filename_%s.pkl' % x, 'rb'))\n",
    "        stemmed_tokens = []\n",
    "        for i in read_file:\n",
    "            file_length = len(i)\n",
    "            new_tokens = []\n",
    "            for post in np.arange(file_length):\n",
    "                new_tokens.append(wordnet_stemmer.lemmatize(i[post]))\n",
    "            stemmed_tokens.append(new_tokens)\n",
    "        pkl.dump(stemmed_tokens, open('stemmed_%s.pkl' % x, 'wb'))\n",
    "        print('Stemmed stemmed_%s.pkl' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on WordNet Stemmed Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1999\n",
      "finished 3999\n",
      "finished 5999\n",
      "finished 7999\n",
      "finished 9999\n",
      "finished 11999\n",
      "finished 13999\n",
      "finished 15999\n",
      "finished 17999\n",
      "finished 19999\n",
      "finished 21999\n",
      "finished 23999\n",
      "finished 25999\n",
      "finished 27999\n",
      "finished 29999\n",
      "finished 31999\n",
      "finished 33999\n",
      "finished 35999\n",
      "finished 37999\n",
      "finished 39999\n",
      "finished 41999\n",
      "finished 43999\n",
      "finished 45999\n",
      "finished 47999\n",
      "finished 49999\n",
      "finished 51999\n",
      "finished 53999\n",
      "finished 55999\n",
      "finished 57999\n",
      "finished 59999\n",
      "finished 61999\n",
      "finished 63999\n",
      "finished 65999\n",
      "finished 67999\n",
      "finished 69999\n",
      "finished 71999\n",
      "finished 73999\n",
      "finished 75999\n",
      "finished 77999\n",
      "finished 79999\n",
      "finished 81999\n",
      "finished 83999\n",
      "finished 85999\n",
      "finished 87999\n",
      "finished 89999\n",
      "finished 91999\n",
      "finished 93999\n",
      "finished 95999\n",
      "finished 97999\n",
      "finished 99999\n",
      "finished 101999\n",
      "finished 103999\n",
      "finished 105999\n",
      "finished 107999\n",
      "finished 109999\n",
      "finished 111999\n",
      "finished 113999\n",
      "finished 115999\n",
      "finished 117999\n",
      "finished 119999\n",
      "finished 121999\n",
      "finished 123999\n",
      "finished 125999\n",
      "finished 127999\n",
      "finished 129999\n",
      "finished 131999\n",
      "finished 133999\n",
      "finished 135999\n",
      "finished 137999\n",
      "finished 139999\n",
      "finished 141999\n",
      "finished 143999\n",
      "finished 145999\n",
      "finished 147999\n",
      "finished 149999\n",
      "finished 151999\n",
      "finished 153999\n",
      "finished 155999\n",
      "finished 157999\n",
      "finished 159999\n",
      "finished 161999\n",
      "finished 163999\n",
      "finished 165999\n",
      "finished 167999\n",
      "finished 169999\n",
      "finished 171999\n",
      "finished 173999\n",
      "finished 175999\n",
      "finished 177999\n",
      "finished 179999\n",
      "finished 181999\n",
      "finished 183999\n",
      "finished 185999\n",
      "finished 187999\n",
      "finished 189999\n",
      "finished 191999\n",
      "finished 193999\n",
      "finished 195999\n",
      "finished 197435\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for x in np.arange(num_posts):\n",
    "    if  ((x + 1) % 2000 == 0) | (x == num_posts - 1):\n",
    "        read_file = pkl.load(open('stemmed_%s.pkl' % x, 'rb'))\n",
    "        for i in read_file:\n",
    "            file_length = len(i)\n",
    "            for word in np.arange(file_length):\n",
    "                texts.append(i[word])\n",
    "        print('finished %s' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow([text]) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrive given stop word set\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of personal pronouns found in given stop word set\n",
    "personal_pronouns = ['i', 'i\\'d', 'i\\'ll', 'i\\'m', 'i\\'ve',\n",
    "                      'me', 'my','myself']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing personal pronouns from stop word array\n",
    "for word in personal_pronouns:\n",
    "    en_stop.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopped tokens\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stemmed texts\n",
    "texts = [p_stemmer.stem(i) for i in stopped_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow([text]) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print LDA model's output, 5 topics\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=5))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
