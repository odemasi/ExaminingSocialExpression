{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk, re\n",
    "import sklearn\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import nltk.data\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = pd.read_csv('depression_posts_full_corpus_201509.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = posts[posts['author'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_texts = posts['selftext'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrive given stop word set\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "#list of personal pronouns found in given stop word set\n",
    "personal_pronouns = ['i', 'i\\'d', 'i\\'ll', 'i\\'m', 'i\\'ve',\n",
    "                      'me', 'my','myself']\n",
    "\n",
    "#removing personal pronouns from stop word array\n",
    "for word in personal_pronouns:\n",
    "    if word in en_stop:\n",
    "        en_stop.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Post Texts for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#punkt tokenizers tokenizes paragraph into sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modified from tutorial on kaggle\n",
    "\n",
    "def sentence_to_wordlist(text):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    #  \n",
    "    # 1. Remove non-letters\n",
    "    text = re.sub(\"[^a-zA-Z\\']\",\" \", text)\n",
    "    #\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 3. Remove Stop Words\n",
    "    words = [w for w in words if not w in en_stop]\n",
    "    #\n",
    "    # 4. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "def post_to_sentences(post):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(post.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_texts = [x for x in posts_texts if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_texts = [post_to_sentences(post) for post in posts_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##important for assigning individual posts their clusters\n",
    "unshuffled_posts = posts_texts.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and KMeans on Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112645\n"
     ]
    }
   ],
   "source": [
    "num_texts = len(posts_texts)\n",
    "print(num_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle posts_texts for some semblance of randomness\n",
    "np.random.shuffle(posts_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating word2vec model\n",
    "initial_clusters = posts_texts[1:2000]\n",
    "sentences = []\n",
    "for post in initial_clusters:\n",
    "    for sentence in post:\n",
    "        sentences.append(sentence)\n",
    "model = word2vec.Word2Vec(sentences, min_count=10)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forming initial clustering on model\n",
    "kmeans_clustering = KMeans(n_clusters=15)\n",
    "word_vectors = model.wv.syn0\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danicatanquilut/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:821: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  n_jobs=self.n_jobs)\n"
     ]
    }
   ],
   "source": [
    "centers = kmeans_clustering.cluster_centers_\n",
    "rest_of_posts = posts_texts[2000:]\n",
    "while rest_of_posts:\n",
    "    sentences = []\n",
    "    current = rest_of_posts[:2000]\n",
    "    for post in current:\n",
    "        for sentence in post:\n",
    "            sentences.append(sentence)\n",
    "    model = word2vec.Word2Vec(sentences, min_count=10)\n",
    "    kmeans_clustering = KMeans(n_clusters = 15, init=centers)\n",
    "    word_vectors = model.wv.syn0\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors) \n",
    "    centers = kmeans_clustering.cluster_centers_\n",
    "    rest_of_posts = rest_of_posts[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Posts by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## borrowed/modified from kaggle\n",
    "def create_bag_of_centroids(post, word_centroid_map):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for sentence in post:\n",
    "        for word in sentence:\n",
    "            if word in word_centroid_map:\n",
    "                index = word_centroid_map[word]\n",
    "                bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = list()\n",
    "\n",
    "#creating bag of centroids for every post\n",
    "for post in unshuffled_posts:\n",
    "    centroids.append(create_bag_of_centroids(post, word_centroid_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.89000000e+02   2.42800000e+03   5.89000000e+02   4.82790000e+04\n",
      "   5.84980000e+04   2.10000000e+01   3.73000000e+02   6.19000000e+02\n",
      "   2.24000000e+02   0.00000000e+00   1.03300000e+03   1.09000000e+02\n",
      "   9.50000000e+01   4.10000000e+01   1.47000000e+02]\n"
     ]
    }
   ],
   "source": [
    "## examining distribution of posts by cluster\n",
    "counter = np.zeros(15)\n",
    "for post in centroids:\n",
    "    index = np.argmax(post)\n",
    "    counter[index] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_clusters = []\n",
    "for post in centroids:\n",
    "    post_clusters.append(np.argmax(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4, 4, 4, 7, 3, 13, 10, 4, 3, 3, 7, 3, 3, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 4, 6, 3, 4, 3, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(post_clusters[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "Determine usefulness of clusters <br>\n",
    "Add clusters to features --> On separate notebook; Find better way to do this? <br>\n",
    "Finish model to predict response rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
